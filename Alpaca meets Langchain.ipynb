{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "# !pip install -q datasets loralib sentencepiece \n",
    "# !pip -q install bitsandbytes accelerate\n",
    "# !pip -q install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use LangChain - HuggingFacePipeline to wrap the custom LLMs. but this only support text2text-generation, text-generation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, warnings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, BitsAndBytesConfig\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. download alpaca natives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010372638702392578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701c4a6b1d0c4fdb9652ec1730d7410c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_card = \"chavinlo/alpaca-native\"\n",
    "device_map = 'auto'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_card)\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "                                            model_card,\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device_map\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=base_model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_length=256,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "{instruction}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        template=template, \n",
    "                        input_variables=[\"instruction\"]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "                     prompt=prompt, \n",
    "                     llm=local_llm\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. experiment on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of England is London.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of England?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alpacas are small, domesticated animals related to camels. They are native to Peru and Bolivia, where they have been used as pack animals for carrying goods on mountain trails. Llamas are also small, but much more common, and are native to South America. Unlike alpacas, they are not domesticated domesticate and do not make good pack animals because of their size.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are alpacas? and how are they different from llamas?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. setting up a chat with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use window memory approach & set window size to 4\n",
    "window_memory = ConversationBufferWindowMemory(k=4)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "                                llm=local_llm, \n",
    "                                verbose=True, \n",
    "                                memory=window_memory\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the original prompt template\n",
    "\n",
    "conversation.prompt.template = '''The following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    " \n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Isuru\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hey there! It's nice to meet you too. What can I help you with?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there! I am Isuru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: Hi there! I am Isuru\n",
      "AI:  Hey there! It's nice to meet you too. What can I help you with?\n",
      "Human: What is your name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' My name is Alpaca. How did you come up with that name?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: Hi there! I am Isuru\n",
      "AI:  Hey there! It's nice to meet you too. What can I help you with?\n",
      "Human: What is your name?\n",
      "AI:  My name is Alpaca. How did you come up with that name?\n",
      "Human: Can you tell me what an Alpaca is?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure. An alpaca is a species of South American camelid mammal. They are typically found in the Andes mountains and live in herds led by an alpha female leader.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you tell me what an Alpaca is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: Hi there! I am Isuru\n",
      "AI:  Hey there! It's nice to meet you too. What can I help you with?\n",
      "Human: What is your name?\n",
      "AI:  My name is Alpaca. How did you come up with that name?\n",
      "Human: Can you tell me what an Alpaca is?\n",
      "AI:  Sure. An alpaca is a species of South American camelid mammal. They are typically found in the Andes mountains and live in herds led by an alpha female leader.\n",
      "Human: How is it different from a Llama?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Alpacas and llamas belong to the same family but they have some differences as well. Alpacas tend to be smaller than llamas and they lack the long ear frills that llamas have. Additionally, their fleece is finer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How is it different from a Llama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 276, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: Hi there! I am Isuru\n",
      "AI:  Hey there! It's nice to meet you too. What can I help you with?\n",
      "Human: What is your name?\n",
      "AI:  My name is Alpaca. How did you come up with that name?\n",
      "Human: Can you tell me what an Alpaca is?\n",
      "AI:  Sure. An alpaca is a species of South American camelid mammal. They are typically found in the Andes mountains and live in herds led by an alpha female leader.\n",
      "Human: How is it different from a Llama?\n",
      "AI:   Alpacas and llamas belong to the same family but they have some differences as well. Alpacas tend to be smaller than llamas and they lack the long ear frills that llamas have. Additionally, their fleece is finer\n",
      "Human: Can you give me some good names for a pet llama?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you give me some good names for a pet llama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 257, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: What is your name?\n",
      "AI:  My name is Alpaca. How did you come up with that name?\n",
      "Human: Can you tell me what an Alpaca is?\n",
      "AI:  Sure. An alpaca is a species of South American camelid mammal. They are typically found in the Andes mountains and live in herds led by an alpha female leader.\n",
      "Human: How is it different from a Llama?\n",
      "AI:   Alpacas and llamas belong to the same family but they have some differences as well. Alpacas tend to be smaller than llamas and they lack the long ear frills that llamas have. Additionally, their fleece is finer\n",
      "Human: Can you give me some good names for a pet llama?\n",
      "AI:  \n",
      "Human: Is your name LLaMA?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' No'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Is your name LLaMA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI called Alpaca. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      " \n",
      "Current conversation:\n",
      "Human: Can you tell me what an Alpaca is?\n",
      "AI:  Sure. An alpaca is a species of South American camelid mammal. They are typically found in the Andes mountains and live in herds led by an alpha female leader.\n",
      "Human: How is it different from a Llama?\n",
      "AI:   Alpacas and llamas belong to the same family but they have some differences as well. Alpacas tend to be smaller than llamas and they lack the long ear frills that llamas have. Additionally, their fleece is finer\n",
      "Human: Can you give me some good names for a pet llama?\n",
      "AI:  \n",
      "Human: Is your name LLaMA?\n",
      "AI:  No\n",
      "Human: What food should I feed my new llama?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Your new llama should eat grass,'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What food should I feed my new llama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
